{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8537b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98709005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5980, 21)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load train.csv & build fusion_df\n",
    "train_df = pd.read_csv(\"../data/raw/train.csv\")\n",
    "train_df[\"id\"] = train_df[\"id\"].astype(int)\n",
    "\n",
    "IMAGE_DIR = Path(\"../data/images\")\n",
    "\n",
    "image_ids = sorted([\n",
    "    int(float(p.stem)) for p in IMAGE_DIR.glob(\"*.png\")\n",
    "])\n",
    "\n",
    "fusion_df = (\n",
    "    train_df[train_df[\"id\"].isin(image_ids)]\n",
    "    .drop_duplicates(subset=\"id\")\n",
    "    .sort_values(\"id\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "assert fusion_df.shape[0] == len(image_ids)\n",
    "assert (fusion_df[\"id\"].values == image_ids).all()\n",
    "\n",
    "fusion_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fb4f084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5980,\n",
       " [PosixPath('../data/images/1200019.0.png'),\n",
       "  PosixPath('../data/images/3600057.0.png'),\n",
       "  PosixPath('../data/images/11200290.0.png')])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build ordered image paths\n",
    "image_paths = [\n",
    "    IMAGE_DIR / f\"{pid}.0.png\" if (IMAGE_DIR / f\"{pid}.0.png\").exists()\n",
    "    else IMAGE_DIR / f\"{pid}.png\"\n",
    "    for pid in fusion_df[\"id\"].values\n",
    "]\n",
    "\n",
    "len(image_paths), image_paths[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fed089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset + transforms\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "class SatelliteImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        return self.transform(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13d69d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5980, 512)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate image embeddings (DETERMINISTIC)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "resnet = models.resnet18(\n",
    "    weights=models.ResNet18_Weights.IMAGENET1K_V1\n",
    ")\n",
    "resnet.fc = nn.Identity()\n",
    "resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "for p in resnet.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "dataset = SatelliteImageDataset(image_paths, image_transform)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        emb = resnet(batch)\n",
    "        all_embeddings.append(emb.cpu().numpy())\n",
    "\n",
    "X_image = np.vstack(all_embeddings)\n",
    "X_image.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e21856fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../data/processed/image_embeddings_fusion.npy\", X_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c10747d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5980, 18), (5980,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare tabular features + target\n",
    "target = \"price\"\n",
    "drop_cols = [\"id\", \"date\", target]\n",
    "\n",
    "X_tabular = fusion_df.drop(columns=drop_cols)\n",
    "y = np.log1p(fusion_df[target].values)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tabular_scaled = scaler.fit_transform(X_tabular)\n",
    "\n",
    "X_tabular_scaled.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47ebd9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5980, 530)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Early fusion\n",
    "X_fusion = np.hstack([X_tabular_scaled, X_image])\n",
    "X_fusion.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa1e6dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4784, 530), (1196, 530))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train / validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_fusion,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train.shape, X_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c365641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_t = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d737be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Fusion MLP\n",
    "class FusionMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d81710c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train MSE: 35.0159 | Val MSE: 3.6175\n",
      "Epoch 2/15 | Train MSE: 2.6681 | Val MSE: 1.8531\n",
      "Epoch 3/15 | Train MSE: 1.4142 | Val MSE: 1.0903\n",
      "Epoch 4/15 | Train MSE: 0.9055 | Val MSE: 0.7709\n",
      "Epoch 5/15 | Train MSE: 0.6583 | Val MSE: 0.6074\n",
      "Epoch 6/15 | Train MSE: 0.5378 | Val MSE: 0.5616\n",
      "Epoch 7/15 | Train MSE: 0.4810 | Val MSE: 0.4840\n",
      "Epoch 8/15 | Train MSE: 0.4345 | Val MSE: 0.4511\n",
      "Epoch 9/15 | Train MSE: 0.4061 | Val MSE: 0.4706\n",
      "Epoch 10/15 | Train MSE: 0.3848 | Val MSE: 0.4318\n",
      "Epoch 11/15 | Train MSE: 0.3723 | Val MSE: 0.4045\n",
      "Epoch 12/15 | Train MSE: 0.3494 | Val MSE: 0.3729\n",
      "Epoch 13/15 | Train MSE: 0.3379 | Val MSE: 0.3640\n",
      "Epoch 14/15 | Train MSE: 0.3378 | Val MSE: 0.3992\n",
      "Epoch 15/15 | Train MSE: 0.3206 | Val MSE: 0.3477\n"
     ]
    }
   ],
   "source": [
    "# Train Fusion Model\n",
    "model = FusionMLP(X_train.shape[1]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train_t, y_train_t),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(X_val_t, y_val_t),\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} | \"\n",
    "        f\"Train MSE: {train_loss:.4f} | \"\n",
    "        f\"Val MSE: {val_loss:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4dbc9d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1196,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate validation predictions\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_preds_log = model(X_val_t.to(device)).cpu().numpy().ravel()\n",
    "\n",
    "val_preds_log.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5679bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to price scale\n",
    "val_preds_price = np.expm1(val_preds_log)\n",
    "val_true_price = np.expm1(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a50631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 426696.0662099379, R2: -0.29377725218330886\n"
     ]
    }
   ],
   "source": [
    "# Compute RMSE & RÂ²\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, r2_score\n",
    "\n",
    "# In scikit-learn >= 1.4, use root_mean_squared_error directly\n",
    "rmse = root_mean_squared_error(\n",
    "    val_true_price,\n",
    "    val_preds_price\n",
    ")\n",
    "\n",
    "r2 = r2_score(\n",
    "    val_true_price,\n",
    "    val_preds_price\n",
    ")\n",
    "\n",
    "print(f\"RMSE: {rmse}, R2: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
